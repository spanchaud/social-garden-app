import streamlit as st
import google.generativeai as genai
import os

# --- 1. CONFIGURATION DE LA PAGE ---
st.set_page_config(page_title="Social Garden", page_icon="üå±")
st.title("Social Garden üå±")
st.write("Bienvenue dans votre espace Social Garden.")

# --- 2. LE CERVEAU DE VOTRE APP (IMPORTANT !) ---
# C'est ici que vous devez coller les instructions que vous aviez dans AI Studio.
# Copiez votre texte entre les trois guillemets ci-dessous.
SYSTEM_PROMPT = """
Tu es Social Garden, un assistant expert en jardinage social et r√©seautage.
Ton but est d'aider l'utilisateur √† cultiver ses relations professionnelles.
R√©ponds toujours de mani√®re bienveillante, encourageante et structur√©e.
Si l'utilisateur pose une question hors sujet, ram√®ne-le doucement au jardinage social.
"""

# --- 3. CONNEXION CL√â API ---
try:
    api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=api_key)
except Exception as e:
    st.error("Erreur de cl√© API. V√©rifiez vos 'Secrets' dans Streamlit.")
    st.stop()

# --- 4. CONFIGURATION DU MOD√àLE ---
# On utilise 'gemini-pro' qui est plus stable pour √©viter l'erreur 404
model = genai.GenerativeModel('gemini-pro') 

# --- 5. INTERFACE DE DISCUSSION ---

# Initialiser l'historique si c'est la premi√®re fois
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "user", "parts": [SYSTEM_PROMPT]}, # On injecte la personnalit√© au d√©but cach√©e
        {"role": "model", "parts": ["Bien compris. Je suis pr√™t √† agir en tant que Social Garden."]}
    ]

# Afficher les anciens messages (sauf le prompt syst√®me cach√©)
for message in st.session_state.messages[2:]:
    with st.chat_message(message["role"]):
        st.markdown(message["parts"][0])

# Zone de saisie pour l'utilisateur
if prompt := st.chat_input("Posez votre question √† Social Garden..."):
    # 1. Afficher le message de l'utilisateur
    st.chat_message("user").markdown(prompt)
    
    # 2. L'ajouter √† l'historique
    st.session_state.messages.append({"role": "user", "parts": [prompt]})
    
    # 3. Demander la r√©ponse √† l'IA
    try:
        chat = model.start_chat(history=st.session_state.messages)
        response = chat.send_message(prompt)
        
        # 4. Afficher la r√©ponse
        with st.chat_message("model"):
            st.markdown(response.text)
            
        # 5. Sauvegarder la r√©ponse
        st.session_state.messages.append({"role": "model", "parts": [response.text]})
        
    except Exception as e:
        st.error(f"Une erreur s'est produite : {e}")
